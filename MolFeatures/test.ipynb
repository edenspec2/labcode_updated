{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/gpfs0/gaus/users/edenspec/.conda/envs/working_env/bin/python\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class ReExpressions(Enum):\n",
    "    FLOAT = r'[-+]?[0-9]*\\.?[0-9]+'\n",
    "    # FLOAT= r'-?\\d*\\.\\d*'\n",
    "    FLOATS_ONLY= \"[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?\\d+)?\"\n",
    "    BONDS= r'R\\(\\d+.\\d+'\n",
    "    FREQUENCY= r'\\-{19}'\n",
    "    CHARGES=r'^-?[0-9]+([.][0-9]+)?$'\n",
    "\n",
    "class FileFlags(Enum):\n",
    "    DIPOLE_START='Dipole moment'\n",
    "    DIPOLE_END='Quadrupole moment'\n",
    "    MAIN_CUTOFF= r'\\-{69}'\n",
    "    STANDARD_ORIENTATION_START='Standard orientation:'\n",
    "    POL_START='iso'\n",
    "    POL_END='xx'\n",
    "    CHARGE_START='Summary of'\n",
    "    CHARGE_END='====='\n",
    "    FREQUENCY_START='Harmonic frequencies'\n",
    "    FREQUENCY_END='Thermochemistry'\n",
    "    MULIKEN_START='Mulliken charges:'\n",
    "    MULIKEN_END='Sum of Mulliken'\n",
    "\n",
    "class Names(Enum):\n",
    "    \n",
    "    DIPOLE_COLUMNS=['dip_x','dip_y','dip_z','total_dipole']\n",
    "    STANDARD_ORIENTATION_COLUMNS=['atom','x','y','z']\n",
    "    DF_LIST=['standard_orientation_df', 'dipole_df', 'pol_df', 'atype_df','charge_df', 'bonds_df', 'info_df','energy_value']\n",
    "\n",
    "\n",
    "class GeneralConstants(Enum):    \n",
    "    ATOMIC_NUMBERS ={\n",
    "     '1':'H', '5':'B', '6':'C', '7':'N', '8':'O', '9':'F', '14':'Si',\n",
    "              '15':'P', '16':'S', '17':'Cl', '35':'Br', '53':'I', '27':'Co', '28':'Ni','46':'Pd', '78':'Pt'}\n",
    "     \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def search_phrase_in_text(text_lines: str, key_phrase: str) : \n",
    "        \"\"\"\n",
    "        Searches for a key phrase in a list of text lines.\n",
    "        \n",
    "        Parameters:\n",
    "            text_lines: text string to search through.\n",
    "            key_phrase: The phrase to search for.\n",
    "            \n",
    "        Returns:\n",
    "            The index of the first line where the key phrase is found, or None if not found.\n",
    "        \"\"\"   \n",
    "        search_result=re.compile(key_phrase).search(text_lines)\n",
    "        # print(f\"Found key phrase '{key_phrase}' at line {search_result.start()}\") if search_result else print(f\"Key phrase '{key_phrase}' not found\")\n",
    "        return search_result\n",
    "\n",
    "def extract_lines_from_text(text_lines, re_expression):\n",
    "    selected_lines=re.findall(re_expression, text_lines)\n",
    "    # if strip:\n",
    "    #     selected_lines=selected_lines.strip()\n",
    "    return selected_lines\n",
    "\n",
    "\n",
    "\n",
    "def process_gaussian_charge_text(log_file_lines):\n",
    "    charges_start =search_phrase_in_text(log_file_lines, key_phrase=FileFlags.CHARGE_START.value)\n",
    "    charges_end = search_phrase_in_text(log_file_lines, key_phrase=FileFlags.CHARGE_END.value)\n",
    "    selected_lines = (extract_lines_from_text(log_file_lines[charges_start.start():charges_end.start()],\n",
    "                                             re_expression=ReExpressions.FLOATS_ONLY.value))\n",
    "    charge_array = np.array(selected_lines)\n",
    "    charge_array=charge_array[1::6]\n",
    "    return pd.DataFrame(charge_array, columns=['charge'])\n",
    "\n",
    "def find_all_matches(log_file_lines, key_phrase):\n",
    "    return [match for match in re.finditer(key_phrase, log_file_lines)]\n",
    "\n",
    "def process_gaussian_dipole_text(log_file_lines):\n",
    "    # Find all occurrences of the start and end markers\n",
    "    dipole_starts = find_all_matches(log_file_lines, FileFlags.DIPOLE_START.value)\n",
    "    dipole_ends = find_all_matches(log_file_lines, FileFlags.DIPOLE_END.value)\n",
    "\n",
    "    if dipole_starts and dipole_ends:\n",
    "        last_dipole_start = dipole_starts[-1].end()\n",
    "        last_dipole_end = dipole_ends[-1].start()\n",
    "        text_section = log_file_lines[last_dipole_start:last_dipole_end]\n",
    "        selected_lines = extract_lines_from_text(text_section, re_expression=ReExpressions.FLOAT.value)\n",
    "        selected_lines = selected_lines[0:4] \n",
    "        dipole_df = pd.DataFrame(selected_lines, index=Names.DIPOLE_COLUMNS.value).T\n",
    "        \n",
    "        return dipole_df\n",
    "\n",
    "def process_muliken_charges(log_file_lines):\n",
    "\n",
    "    muliken_start = find_all_matches(log_file_lines, FileFlags.MULIKEN_START.value)\n",
    "    muliken_end = find_all_matches(log_file_lines, FileFlags.MULIKEN_END.value)\n",
    "    if muliken_start and muliken_end:\n",
    "        \n",
    "        last_muliken_start = muliken_start[-1].end()\n",
    "        last_muliken_end = muliken_end[-1].start()\n",
    "        \n",
    "        text_section = log_file_lines[last_muliken_start:last_muliken_end]\n",
    "        \n",
    "        charges_re = re.findall(r'-?\\d+\\.\\d+', text_section)\n",
    "        \n",
    "        charges_float = [float(charge) for charge in charges_re]\n",
    "        \n",
    "        charge_df = pd.DataFrame(charges_float, columns=['charge'])\n",
    "       \n",
    "        return charge_df\n",
    "        \n",
    "\n",
    "def gauss_first_split(log_file_lines):\n",
    "    first_split=re.split(FileFlags.STANDARD_ORIENTATION_START.value,log_file_lines)[-1]\n",
    "    gauss_data=re.split(FileFlags.MAIN_CUTOFF.value,first_split)\n",
    "    return gauss_data\n",
    "\n",
    "def process_gaussian_standard_orientation_text(log_file_lines):\n",
    "\n",
    "    standard_orientation_lines=(extract_lines_from_text(log_file_lines, ReExpressions.FLOATS_ONLY.value ))\n",
    "    standard_orientation=np.array(standard_orientation_lines).reshape(-1,6)\n",
    "    \n",
    "    standard_orientation=np.delete(standard_orientation,(0,2),1)\n",
    "    standard_orientation_df=pd.DataFrame(standard_orientation,columns=Names.STANDARD_ORIENTATION_COLUMNS.value)\n",
    "    \n",
    "    standard_orientation_df.replace({'atom': GeneralConstants.ATOMIC_NUMBERS.value}, inplace=True)\n",
    "    # standard_orientation_df[['x', 'y', 'z']] = standard_orientation_df[['x', 'y', 'z']].astype(float)\n",
    "    ## add 2 columns on top and add the .shape[0] to the first row 'atom' column\n",
    "    # Create a new DataFrame with two rows\n",
    "    new_rows = pd.DataFrame(np.nan, index=[0, 1], columns=standard_orientation_df.columns)\n",
    "    # Set the 'atom' column in the first row to the number of rows in df\n",
    "    new_rows.loc[0, 'atom'] = standard_orientation_df.shape[0]\n",
    "    # Concatenate the new rows with df\n",
    "    \n",
    "    df = pd.concat([new_rows, standard_orientation_df], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# def process_gaussian_pol(log_file_lines):\n",
    "#     pol_start=search_phrase_in_text(log_file_lines, key_phrase=FileFlags.POL_START.value)\n",
    "#     pol_end=search_phrase_in_text(log_file_lines, key_phrase=FileFlags.POL_END.value)\n",
    "#     pol=extract_lines_from_text(log_file_lines[pol_start.start():pol_end.start()], re_expression=ReExpressions.FLOAT.value)\n",
    "#     pol_df=pd.DataFrame([float(pol[0])*1000,float(pol[3])*1000],index=['iso','aniso'],dtype=float)\n",
    "#     return pol_df\n",
    "\n",
    "def search_phrase_in_text_pol(text: str, key_phrase: str) -> int:\n",
    "    \"\"\"\n",
    "    Search for a key phrase in a text string and return the start position of the match.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to search within.\n",
    "        key_phrase (str): The phrase to search for.\n",
    "\n",
    "    Returns:\n",
    "        int: The start index of the first match or None if not found.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(rf\"{re.escape(str(key_phrase))}\\s\")\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return match.start()\n",
    "    return None\n",
    "\n",
    "def process_gaussian_pol_text(log_file_lines: List[str]) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Processes Gaussian polarization data and returns a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        log_file_lines: List of lines from the Gaussian log file.\n",
    "        \n",
    "    Returns:\n",
    "        A DataFrame containing polarization data or None if not found.\n",
    "    \"\"\"\n",
    "    # If log_file_lines is a list, convert it to a single string\n",
    "    if isinstance(log_file_lines, list):\n",
    "        log_file_text = '\\n'.join(log_file_lines)\n",
    "    else:\n",
    "        log_file_text = log_file_lines\n",
    "\n",
    "    pol_start = search_phrase_in_text_pol(log_file_text, key_phrase=FileFlags.POL_START.value)\n",
    "    pol_end = search_phrase_in_text_pol(log_file_text, key_phrase=FileFlags.POL_END.value)\n",
    "    \n",
    "    if pol_start is not None and pol_end is not None:\n",
    "        pol = extract_lines_from_text(log_file_lines[pol_start:pol_end], re_expression=ReExpressions.FLOAT.value)\n",
    "        pol_df = pd.DataFrame([float(pol[0])*1000, float(pol[6])*1000], index=['iso', 'aniso'], dtype=str).T\n",
    "        return pol_df\n",
    "    else:\n",
    "        # print(\"Failed to create.\")\n",
    "        pol_df = pd.DataFrame([100, 100], index=['iso', 'aniso'], dtype=str).T\n",
    "        return pol_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_gaussian_bonds(log_file_lines):\n",
    "    bonds=extract_lines_from_text(log_file_lines, re_expression=ReExpressions.BONDS.value)\n",
    "    bonds_text=[re.sub(r'R\\(','',line).split(',') for line in bonds]\n",
    "    return bonds_text\n",
    "\n",
    "\n",
    "\n",
    "def remove_floats_until_first_int(input_list):\n",
    "    output_list = []\n",
    "    encountered_integer = False\n",
    "    for item in input_list:\n",
    "        try:\n",
    "            # Try converting the string to an integer\n",
    "            int_item = int(item)\n",
    "            encountered_integer = True\n",
    "            output_list.append(item)\n",
    "        except ValueError:\n",
    "            # If conversion to integer fails, try float\n",
    "            try:\n",
    "                float_item = float(item)\n",
    "                if encountered_integer:\n",
    "                    output_list.append(item)\n",
    "            except ValueError:\n",
    "                # If conversion to both integer and float fails, keep the item\n",
    "                output_list.append(item)\n",
    "    return output_list\n",
    "def parse_dipole_and_polarizability(data):\n",
    "    # Adjusted regex pattern\n",
    "\n",
    "    dipole_pattern = re.compile(r'VibFq2-Diag2([\\s\\S]*?)Diagonal vibrational')\n",
    "    match = dipole_pattern.findall(data)[0]\n",
    "    \n",
    "    pattern = re.compile(\n",
    "        r':\\s+N=\\s+\\d+\\s+IV=\\s+\\d+\\s+MDV=\\s+\\d+\\s+NE2=\\s+\\d+([\\s\\S]*?)(?=:\\s+N=|\\Z)',\n",
    "        re.MULTILINE\n",
    "    )\n",
    "    \n",
    "    matches = pattern.findall(match)\n",
    "   \n",
    "    \n",
    "    if not matches:\n",
    "        raise ValueError(\"No dipole and polarizability data found in the input.\")\n",
    "    \n",
    "    # Use a set to track unique blocks\n",
    "    unique_blocks = set()\n",
    "    \n",
    "    dipole_derivatives = []\n",
    "    polarizability_contributions = []\n",
    "    \n",
    "    for block in matches:\n",
    "        block = block.strip()\n",
    "        \n",
    "        # Skip if this block has already been processed\n",
    "        if block in unique_blocks:\n",
    "            continue\n",
    "        unique_blocks.add(block)\n",
    "        \n",
    "        # Process each line in the block\n",
    "        lines = block.splitlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            # Extract dipole derivative values\n",
    "            if line.startswith(\"Dipole derivative\"):\n",
    "                dipole_values = line.split(\":\")[1].strip().split()\n",
    "                dipole_derivatives.append([float(v.replace(\"D\", \"E\")) for v in dipole_values])\n",
    "            # Extract vibrational polarizability contributions\n",
    "            elif line.startswith(\"Vibrational polarizability contributions from mode\"):\n",
    "                polarizability_values = line.split()[-3:]\n",
    "                polarizability_contributions.append([float(v.replace(\"D\", \"E\")) for v in polarizability_values])\n",
    "    \n",
    "    # Convert lists to DataFrames and remove duplicates\n",
    "    dipole_derivatives_df = pd.DataFrame(dipole_derivatives, columns=['X', 'Y', 'Z']).drop_duplicates().reset_index(drop=True)\n",
    "    polarizability_contributions_df = pd.DataFrame(polarizability_contributions, columns=['Alpha_X', 'Alpha_Y', 'Alpha_Z']).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    return dipole_derivatives_df, polarizability_contributions_df\n",
    "\n",
    "# def parse_dipole_and_polarizability(data):\n",
    "#     # Regex to extract the relevant section\n",
    "#     dipole_pattern = re.compile(r'VibFq2-Diag2([\\s\\S]*?)Diagonal vibrational')\n",
    "#     match = dipole_pattern.findall(data)[0]\n",
    "    \n",
    "#     if not match:\n",
    "#         raise ValueError(\"No dipole and polarizability data found in the input.\")\n",
    "   \n",
    "#     relevant_section = match  # Extract the matched section\n",
    "    \n",
    "#     dipole_derivatives = []\n",
    "#     polarizability_contributions = []\n",
    "\n",
    "#     # Process each line of the extracted section\n",
    "#     lines = relevant_section.splitlines()\n",
    "#     for i, line in enumerate(lines):\n",
    "#         line = line.strip()\n",
    "#         # Extract dipole derivative values\n",
    "#         if line.startswith(\"Dipole derivative\"):\n",
    "#             dipole_values = line.split(\":\")[1].strip().split()\n",
    "#             dipole_derivatives.append([float(v.replace(\"D\", \"E\")) for v in dipole_values])\n",
    "           \n",
    "#         # Extract vibrational polarizability contributions\n",
    "#         elif line.startswith(\"Vibrational polarizability contributions from mode\"):\n",
    "#             polarizability_values = line.split()[-3:]\n",
    "#             polarizability_contributions.append([float(v.replace(\"D\", \"E\")) for v in polarizability_values])\n",
    "\n",
    "#     # Convert to numpy arrays\n",
    "#     dipole_derivatives = pd.DataFrame(dipole_derivatives).drop_duplicates()\n",
    "#     polarizability_contributions = pd.DataFrame(polarizability_contributions).drop_duplicates()\n",
    "#     print( dipole_derivatives[0])\n",
    "#     return dipole_derivatives, polarizability_contributions\n",
    "\n",
    "def process_gaussian_vibs_string(log_file_lines):\n",
    "    pattern = re.compile(rf'{FileFlags.FREQUENCY_START.value}([\\s\\S]*?){FileFlags.FREQUENCY_END.value}')\n",
    "    match = pattern.search(log_file_lines)\n",
    "    if match:\n",
    "        vibration_section = match.group(1).strip()\n",
    "\n",
    "        # Further slicing the obtained text based on 'Frequencies --'\n",
    "        frequencies_blocks = re.split(r'Frequencies -- ', vibration_section)\n",
    "\n",
    "        # Removing the last part that contains '------'\n",
    "        final_blocks = [block.split('-------------------')[0].strip() for block in frequencies_blocks[1:]]\n",
    "\n",
    "        return final_blocks\n",
    "    \n",
    "def process_gaussian_info(frequency_string):\n",
    "    mass,frc,frequency,ir=[],[],[],[]\n",
    "    \n",
    "    for i,data in enumerate(frequency_string):\n",
    "        \n",
    "        match=extract_lines_from_text(data, re_expression=ReExpressions.FLOATS_ONLY.value)\n",
    "        # if i == len(frequency_string) - 1:\n",
    "        #     frequency.append([match[0]])\n",
    "        #     frc.append([match[1]])\n",
    "        #     mass.append([match[2]])\n",
    "        #     ir.append([match[3]])\n",
    "        # else:\n",
    "        frequency.append((match[0:3]))\n",
    "        frc.append(match[6:9])\n",
    "        mass.append(match[3:6])\n",
    "        ir.append(match[9:12])\n",
    "        \n",
    "        \n",
    "    \n",
    "    info_df=pd.DataFrame()\n",
    "    info_df['Frequency']=[(item) for sublist in frequency for item in sublist]\n",
    "    info_df['IR']=[(item) for sublist in ir for item in sublist] \n",
    "    info_df['Force']=[(item) for sublist in frc for item in sublist]\n",
    "    info_df['Mass']=[(item) for sublist in mass for item in sublist]\n",
    "    \n",
    "    return info_df\n",
    "\n",
    "def vib_array_list_to_df(array_list):\n",
    "    array_list_df=[]\n",
    "\n",
    "    for arrays in array_list:\n",
    "        new_df=[]\n",
    "        for array in arrays:\n",
    "            new_array=np.delete(array,[0,1]).reshape(-1,3)\n",
    "            new_df.append(new_array)\n",
    "        new_df=pd.DataFrame(np.vstack(new_df))\n",
    "        array_list_df.append(new_df)\n",
    "    vibs_df=pd.concat(array_list_df,axis=1)\n",
    "    return vibs_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def are_first_two_int_strings(values):\n",
    "    # Convert to string and check if they are digit strings that can be converted to integers\n",
    "    try:\n",
    "        # Using int conversion to check; this handles negative and positive integers in string form\n",
    "        all(isinstance(int(value), int) for value in values[:2])\n",
    "        return True\n",
    "    except ValueError:\n",
    "        # If conversion fails, then not all are integer strings\n",
    "        return False\n",
    "    \n",
    "def process_gaussian_frequency_string(final_blocks):\n",
    "    vibs_list=[]\n",
    "    short_list=[]\n",
    "    lenght=[]\n",
    "    for i,data in enumerate(final_blocks):\n",
    "            match=re.findall(ReExpressions.FLOATS_ONLY.value,data)\n",
    "            # if i == len(final_blocks) - 1:\n",
    "            #     del match[0:4]\n",
    "            # else:\n",
    "            del match[0:12] \n",
    "            match=np.array(match)\n",
    "           ## need to find a way to deal with last match which is not a multiple of 11, planar vibrations\n",
    "            # match=remove_floats_until_first_int(match)\n",
    "            match=np.array(match)\n",
    "            if i==1:\n",
    "                match=remove_floats_until_first_int(match)\n",
    "            match=np.array(match)\n",
    "            lenght.append(len(match))\n",
    "            try:\n",
    "                # print(match.reshape(-1,11))\n",
    "                vibs_list.append(match.reshape(-1,11))  \n",
    "            except ValueError:   \n",
    "                    try:\n",
    "                        match_try=np.delete(match,[-1],0)\n",
    "                        # print(match_try)\n",
    "                        vibs_list.append(np.array(match_try).reshape(-1,11))\n",
    "                    except Exception as e:\n",
    "                        try: \n",
    "                            \n",
    "                            match_r=np.delete(match,[-1,-2,-3],0)\n",
    "                            # print(match_r)\n",
    "                            vibs_list.append(np.array(match_r).reshape(-1,11))\n",
    "                        except Exception as e:\n",
    "                            raise ValueError(f\"Error processing vibrations: {e}\")\n",
    "                                \n",
    "    vibs=np.vstack(vibs_list)\n",
    "    \n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "    final_atom=vibs[-1][0]\n",
    "    ordered_vibs=[]\n",
    "    for i in range(1,int(final_atom)+1):\n",
    "        ordered_vibs.append(vibs[vibs[:,0]==str(i)])\n",
    "    # print(ordered_vibs)\n",
    "    # ordered_vibs_list=[ordered_vibs[i:i + len(vibs_list)] for i in range(0, len(ordered_vibs), len(vibs_list))]\n",
    "    # print(ordered_vibs_list)\n",
    "    vibs_df=vib_array_list_to_df(ordered_vibs) if ordered_vibs else None\n",
    "    # vibs_df=vib_array_list_to_df_5(ordered_vibs_list,new_arrays) if ordered_vibs_list else None\n",
    "    \n",
    "    return vibs_df \n",
    "\n",
    "\n",
    "def df_list_to_dict(df_list):\n",
    "    my_dict={}\n",
    "    for name,df in zip(Names.DF_LIST.value,df_list):\n",
    "        my_dict[name]=df\n",
    "    return my_dict\n",
    "\n",
    "\n",
    "\n",
    "def process_gaussian_energy_text(energy_string):\n",
    "    cut=re.split('SCF Done', energy_string)[1]\n",
    "    energy=extract_lines_from_text(cut, re_expression=ReExpressions.FLOATS_ONLY.value)\n",
    "    data = np.array([[energy[1]]])\n",
    "    df = pd.DataFrame(data, columns=['energy'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def gauss_file_handler(gauss_filename, export=False):\n",
    "    string_report=''\n",
    "    with open(os.path.abspath(gauss_filename)) as f:\n",
    "        log_file_lines = f.read()\n",
    "\n",
    "    try:\n",
    "        energy_df = process_gaussian_energy_text(log_file_lines)\n",
    "    except IndexError:\n",
    "        energy_df = pd.DataFrame()\n",
    "        print(\"{gauss_filename}: Error processing energy.\")\n",
    "        string_report+=\"{gauss_filename}: Error processing energy.\\n\"\n",
    "\n",
    "    try:\n",
    "        charge_df = process_gaussian_charge_text(log_file_lines)\n",
    "    except Exception as e:\n",
    "        charge_df = pd.DataFrame()\n",
    "        print(f\"{gauss_filename}: Error processing charge: {e}\")\n",
    "        string_report+=f\"{gauss_filename}: Error processing charge: {e}\\n\"\n",
    "    try:\n",
    "        dipole_df = process_gaussian_dipole_text(log_file_lines)\n",
    "    except Exception as e:\n",
    "        dipole_df = pd.DataFrame()\n",
    "        print(f\"{gauss_filename}: Error processing dipole: {e}\")\n",
    "        string_report+=f\"{gauss_filename}: Error processing dipole: {e}\\n\"\n",
    "    try:\n",
    "        pol_df = process_gaussian_pol_text(log_file_lines)\n",
    "    except Exception as e:\n",
    "        pol_df = pd.DataFrame()\n",
    "        print(f\"{gauss_filename}: Error processing polarization: {e}\")\n",
    "        string_report+=f\"{gauss_filename}: Error processing polarization: {e}\\n\"\n",
    "    try:\n",
    "        gauss_data = gauss_first_split(log_file_lines)\n",
    "        standard_orientation_df = process_gaussian_standard_orientation_text(gauss_data[2])\n",
    "    except Exception as e:\n",
    "        standard_orientation_df = pd.DataFrame()\n",
    "        print(f\"{gauss_filename}: Error processing standard orientation: {e}\")\n",
    "        string_report+=f\"{gauss_filename}: Error processing standard orientation: {e}\\n\"\n",
    "    try:\n",
    "        frequency_str = process_gaussian_vibs_string(log_file_lines)\n",
    "    except Exception as e:\n",
    "        frequency_str = pd.DataFrame()\n",
    "        print(f\"{gauss_filename}: Error processing vibrations: {e}\")\n",
    "        string_report+=f\"{gauss_filename}: Error processing vibrations: {e}\\n\"\n",
    "    try:\n",
    "        info_df = process_gaussian_info(frequency_str)\n",
    "        \n",
    "    except Exception as e:\n",
    "        info_df = pd.DataFrame()  # or some default DataFrame\n",
    "        print(f\"{gauss_filename}: Error processing info: {e}\")\n",
    "        string_report+=f\"{gauss_filename}: Error processing info: {e}\\n\"\n",
    "\n",
    "    try:\n",
    "        vibs_df = process_gaussian_frequency_string(frequency_str)\n",
    "    except Exception as e:\n",
    "        vibs_df = pd.DataFrame() # or some default DataFrame\n",
    "        print(f\"{gauss_filename}: Error processing frequency: {e}\")\n",
    "        string_report+=f\"{gauss_filename}: Error processing frequency: {e}\\n\"\n",
    "    try:\n",
    "        concatenated_df = pd.concat([standard_orientation_df, dipole_df, pol_df, charge_df, info_df, vibs_df, energy_df], axis=1)\n",
    "    except Exception as e:\n",
    "        concatenated_df = pd.DataFrame()  # or some default DataFrame\n",
    "        print(f\"{gauss_filename}: Error concatenating data: {e}\")\n",
    "        string_report+=f\"{gauss_filename}: Error concatenating data: {e}\\n\"\n",
    "\n",
    "    return concatenated_df, string_report\n",
    "\n",
    "def gauss_file_handler_vibs(gauss_filename, export=False):\n",
    "    string_report=''\n",
    "    \n",
    "    \n",
    "    with open(os.path.abspath(gauss_filename)) as f:\n",
    "        log_file_lines = f.read()\n",
    "    try:\n",
    "        gauss_data = gauss_first_split(log_file_lines)\n",
    "        standard_orientation_df = process_gaussian_standard_orientation_text(gauss_data[2])\n",
    "    except Exception as e:\n",
    "        standard_orientation_df = pd.DataFrame()\n",
    "        print(f\"{gauss_filename}: Error processing standard orientation: {e}\")\n",
    "        string_report+=f\"{gauss_filename}: Error processing standard orientation: {e}\\n\"\n",
    "    try:\n",
    "        dipole,polar=parse_dipole_and_polarizability(log_file_lines)\n",
    "        # print(dipole)\n",
    "        # print(polar)\n",
    "    except Exception as e:\n",
    "        dipole = pd.DataFrame()\n",
    "        polar=pd.DataFrame()\n",
    "        print(f\"{gauss_filename}: Error processing dipole and polarizability: {e}\")\n",
    "        string_report+=f\"{gauss_filename}: Error processing dipole and polarizability: {e}\\n\"\n",
    "    try:\n",
    "        frequency_str = process_gaussian_vibs_string(log_file_lines)\n",
    "    except Exception as e:\n",
    "        frequency_str = pd.DataFrame()\n",
    "        print(f\"{gauss_filename}: Error processing vibrations: {e}\")\n",
    "        string_report+=f\"{gauss_filename}: Error processing vibrations: {e}\\n\"\n",
    "    try:\n",
    "        energy_df = process_gaussian_energy_text(log_file_lines)\n",
    "    except Exception as e:\n",
    "        energy_df = pd.DataFrame()\n",
    "        print(\"{gauss_filename}: Error processing energy.\")\n",
    "        string_report+=\"{gauss_filename}: Error processing energy: {e}\\n\"\n",
    "    try:\n",
    "        info_df = process_gaussian_info(frequency_str)\n",
    "        \n",
    "    except Exception as e:\n",
    "        info_df = pd.DataFrame()  # or some default DataFrame\n",
    "        print(f\"{gauss_filename}: Error processing info: {e}\")\n",
    "        string_report+=f\"{gauss_filename}: Error processing info: {e}\\n\"\n",
    "    try:\n",
    "        muliken_df = process_muliken_charges(log_file_lines)\n",
    "    except Exception as e:\n",
    "        muliken_df = pd.DataFrame()  # or some default DataFrame\n",
    "        print(f\"{gauss_filename}: Error processing muliken charges: {e}\")\n",
    "        string_report+=f\"{gauss_filename}: Error processing muliken charges: {e}\\n\"\n",
    "    try:\n",
    "        vibs_df = process_gaussian_frequency_string(frequency_str)\n",
    "        print(vibs_df)\n",
    "    # print(vibs_df)\n",
    "    except Exception as e:\n",
    "        vibs_df = pd.DataFrame() # or some default DataFrame\n",
    "        \n",
    "    #     print(f\"{gauss_filename}: Error processing frequency: {e}\")\n",
    "    #     string_report+=f\"{gauss_filename}: Error processing frequency: {e}\\n\"\n",
    "\n",
    "    try:\n",
    "        concatenated_df = pd.concat([energy_df,standard_orientation_df,muliken_df, info_df,dipole,polar, vibs_df], axis=1)\n",
    "    except Exception as e:\n",
    "        concatenated_df = pd.DataFrame()  # or some default DataFrame\n",
    "        print(f\"{gauss_filename}: Error concatenating data: {e}\")\n",
    "        string_report+=f\"{gauss_filename}: Error concatenating data: {e}\\n\"\n",
    "   \n",
    "    return concatenated_df, string_report\n",
    "\n",
    "\n",
    "\n",
    "def save_to_feather(df, filename):\n",
    "    # Create a DataFrame from the list of strings\n",
    "    # Each string becomes a column. Since all columns must be of the same length,\n",
    "    # you may need to handle this if your strings are of different lengths.\n",
    "    # Define column names that correspond to the data in string_list\n",
    "    # column_names = ['Standard_Orientation', 'Dipole', 'Polarizability', 'Frequency', 'Charge', 'Energy']\n",
    "\n",
    "    \n",
    "    feather_filename = filename + '.feather'\n",
    "    df=df.astype(str)\n",
    "    # Set each column name to a string representation of its index\n",
    "    df.columns = range(df.shape[1]) # [str(i) for i in range(df.shape[1])]\n",
    "    df.columns = df.columns.map(str)\n",
    "    df.to_feather(feather_filename)\n",
    "\n",
    "    print(f\"Data saved to {feather_filename}\")\n",
    "    string_report=f\"Data saved to {feather_filename}\\n\"\n",
    "    return string_report\n",
    "\n",
    "def save_to_feather_vib(df, filename):\n",
    "    # Create a DataFrame from the list of strings\n",
    "    # Each string becomes a column. Since all columns must be of the same length,\n",
    "    # you may need to handle this if your strings are of different lengths.\n",
    "    # Define column names that correspond to the data in string_list\n",
    "    # column_names = ['Standard_Orientation', 'Dipole', 'Polarizability', 'Frequency', 'Charge', 'Energy']\n",
    "\n",
    "    \n",
    "    feather_filename = filename +'_vib'+ '.feather'\n",
    "    df=df.astype(str)\n",
    "    # Set each column name to a string representation of its index\n",
    "    df.columns = range(df.shape[1]) # [str(i) for i in range(df.shape[1])]\n",
    "    df.columns = df.columns.map(str)\n",
    "    df.columns = df.columns.astype(str)\n",
    "    df=df.reset_index(drop=True)\n",
    "    \n",
    "    try:\n",
    "        df.to_feather(feather_filename)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to feather: {e}\")\n",
    "        return f\"Error saving to feather: {e}\\n\"\n",
    "    # print(f\"Data saved to {feather_filename}\")\n",
    "    string_report=f\"Data saved to {feather_filename}\\n\"\n",
    "    return string_report\n",
    "\n",
    "def logs_to_feather(dir_path):\n",
    "    string_report=''\n",
    "    failed_files_string='Files with Errors created with missing DataFrames-\\n'\n",
    "    os.chdir(dir_path)\n",
    "    if not os.path.exists('feather_files'):\n",
    "        os.mkdir('feather_files')\n",
    "\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(\".log\"):\n",
    "            try:\n",
    "                df, gauss_string_report = gauss_file_handler(file)\n",
    "                string_report+=gauss_string_report\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "                string_report+=f\"Error processing file {file}: {e}\\n\"\n",
    "                failed_files_string+=f\"{file}\\n\"\n",
    "                continue  # Skip to the next file\n",
    "\n",
    "            os.chdir('feather_files')\n",
    "            string_report+=save_to_feather(df, file.split('.')[0])  # Assuming you want to remove the .log extension\n",
    "            os.chdir('..')\n",
    "        else:\n",
    "            continue\n",
    "    string_report+=failed_files_string + 'Check the log files and reported errors for more information.'\n",
    "    os.chdir(dir_path)\n",
    "    return string_report\n",
    "\n",
    "def logs_to_feather_vib(dir_path):\n",
    "    string_report = ''\n",
    "    failed_files_string = 'Files with Errors created with missing DataFrames-\\n'\n",
    "    os.chdir(dir_path)\n",
    "    if not os.path.exists('vib_files'):\n",
    "        os.mkdir('vib_files')\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(\".log\"):\n",
    "        # try:\n",
    "            feather_file_path = os.path.join('vib_files', file.replace('.log', '.feather'))\n",
    "            if os.path.exists(feather_file_path):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                df, gauss_string_report = gauss_file_handler_vibs(file)\n",
    "                os.chdir('vib_files')\n",
    "                string_report += save_to_feather_vib(df, file.split('.')[0])\n",
    "                os.chdir('..')\n",
    "                string_report += gauss_string_report\n",
    "                print(f\"Processed file {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "                string_report += f\"Error processing file {file}: {e}\\n\"\n",
    "                failed_files_string += f\"{file}\\n\"\n",
    "                continue  # Skip to the next file\n",
    "            os.chdir('vib_files')\n",
    "            string_report += save_to_feather_vib(df, file.split('.')[0])\n",
    "            os.chdir('..')\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    string_report += failed_files_string + 'Check the log files and reported errors for more information.'\n",
    "    os.chdir(dir_path)\n",
    "    return string_report\n",
    "\n",
    "def data_to_xyz(dataframe, output_name, comment_line=''):\n",
    "    \"\"\"\n",
    "\n",
    "     a function that recieves a dataframe, output name, and comment line and creates a xyz type file.\n",
    "     \n",
    "    parameters\n",
    "\n",
    "    ---\n",
    "\n",
    "    dataframe: an array that can contain different classes of data, needs to be 4 colums to run.\n",
    "\n",
    "    output_name:str, the name for the file created.\n",
    "\n",
    "    comment_line: str, the headline of the file .\n",
    "    ---\n",
    "\n",
    "    examples:\n",
    "    ---\n",
    "    \"\"\"\n",
    "    if type(dataframe) == pd.DataFrame :\n",
    "        number_of_atoms=dataframe.shape[0]\n",
    "        atoms_np_array=dataframe.to_numpy()\n",
    "    else:\n",
    "        number_of_atoms=len(dataframe)\n",
    "        atoms_np_array=dataframe\n",
    "    with open(output_name, 'w') as xyz_file:\n",
    "        for atom_np_array in atoms_np_array:\n",
    "            try:\n",
    "                xyz_file.write(\"{:1} {:11.6} {:11.6} {:11.6} \\n\".format(*atom_np_array))\n",
    "            except:\n",
    "                xyz_file.write(\"{:1}\".format(*atom_np_array))\n",
    "\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "def logs_to_xyz_geom(dir_path):\n",
    "    files=os.listdir(dir_path)\n",
    "    for file in files :\n",
    "        if file.endswith(\".log\"):\n",
    "            with open(file) as f:\n",
    "                log_file_lines=f.read()\n",
    "                f.close()\n",
    "            gauss_data=gauss_first_split(log_file_lines)\n",
    "            try:\n",
    "                standard_orientation_str=process_gaussian_standard_orientation_text(gauss_data[2])\n",
    "                data_to_xyz(standard_orientation_str, file.split('.')[0]+'.xyz', comment_line='')\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "    try:\n",
    "        os.mkdir('xyz_files_geom')\n",
    "    except:\n",
    "        pass\n",
    "    # Move all .xyz files\n",
    "    for file in glob.glob('*.xyz'):\n",
    "        shutil.move(file, 'xyz_files_geom')\n",
    "    \n",
    "\n",
    "# # Example usage\n",
    "path=(r'C:\\Users\\edens\\Documents\\QM9dataset\\test')\n",
    "os.chdir(path)\n",
    "logs_to_feather_vib(path)\n",
    "\n",
    "# for dir in os.listdir():\n",
    "#     dir_path=os.path.join(path,dir)\n",
    "#     os.chdir(dir_path)\n",
    "#     os.chdir('com')\n",
    "#     new_path=os.getcwd()\n",
    "#     logs_to_feather_vib(new_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
